{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Data2VecAudioModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasser.attia/.conda/envs/SNN/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "encoder = Data2VecAudioModel.from_pretrained(\"m-a-p/music2vec-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZ=30000 #Audio Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LI=[\"VØJ, Narvent - Memory Reboot (4K Music Video)\",\"Üsküdar_a Gider İken\",\"Symphony\",\"Toss a Coin to Your Witcher\",\"Smooth Criminal\",\"The good the bad and the ugly\"\n",
    "    ,\"Stronghold - CastleJam\",\"Señorita\",\"Morrison Merrily Kissed The Quaker\",\"Saad Lamjarred - ADDA ELKALAM\", \"Ahd Al Asdikaa\",\"All I Want - Kodaline\",\"Amr Diab - Qusad Einy\",\"Bella Ciao\"\n",
    "    ,\"Fly Me To The Moon\",\"Hamza Namira - Dari Ya Alby\",\"Ladies of the Woods\", \"Lady Gaga - Shallow\",\"Lady Gaga- Rain On Me\",\"Memories - Maroon 5\",\"Attention\",\"GOT\"\n",
    "    ,\"Skrillex - Scary Monsters And Nice Sprites\",\"They_Donot_Care_About_Us\",\"The Misty Mountains Cold - The Hobbit\",\"Fluxxwave\",\"Aria Math (Minecraft) - C418\",\"Mareux - The Perfect Girl\" \n",
    "    ,\"METAMORPHOSIS\",\"Dusk Till Dawn - ZAYN Ft. Sia\",\"Let It Happen - Tame Impala\",\"Interstellar Main Theme - Hans Zimmer\",\"As It Was - Harry Styles\",\"Crystal Castles - KEROSEN\",\"Life in Rio\"    \n",
    "    ,\"Daft Punk - Instant Crush\",\"Fine Line - Harry Styles\",\"Numb - Linkin Park\",\"Sparky Deathcap - September\",\"SLOW DANCING IN THE DARK\",\"Faded - Alan Walker\",\"John Legend - All of Me\"\n",
    "    ,\"Nour El Ain - Amr Diab\",\"Rockabye\",\"Sia - The Greatest\",\"Sidi Mansour\" ,\"The Last Of The Mohicans\",\"The Sound Of Silence\",\"Dance Monkey\",\"Desert Rose - Sting & Cheb Mami\"\n",
    "    ,\"Despacito\",\"Don_t Stop _Til You Get Enough\",\"Ed Sheeran - Perfect\",\"Ed Sheeran - Shape Of You\",\"HOME - Resonance\",\"Interstellar Theme\"\n",
    "    ,\"Adele - Hello\",\"Aicha - Cheb Khaled\",\"Blinding Lights\",\"Charlie Puth - We Don_t Talk Anymore\",\"Cheap Thrills - Sia (Oud cover)\",\"Closer - The Chainsmokers\",\"Coffin Dance\",\n",
    "    \"Story\"]\n",
    "dir=\"/home/yasser.attia/Music_Project/mm/\"\n",
    "sampling_rate=16000\n",
    "segments1=[]\n",
    "segments2=[]\n",
    "#,\"Adele - Hello\",\"John Legend - All of Me\",\"Cheap Thrills - Sia (Oud cover)\",\"Closer - The Chainsmokers\",\"Sia - The Greatest\",\n",
    "#    \"Faded - Alan Walker\",\"Rockabye\",\"Ed Sheeran - Shape Of You\"\n",
    "summ=0\n",
    "for pa in LI:\n",
    "    print(pa)\n",
    "    pa_ar=dir+pa+\"_Arabic\"\n",
    "    pa=dir+pa+\".wav\"\n",
    "    pa_ar=pa_ar+\".wav\"\n",
    "\n",
    "    y1, sr = librosa.load(pa, sr=sampling_rate)  # Load audio with a specific sampling rate\n",
    "\n",
    "\n",
    "    y2, sr = librosa.load(pa_ar, sr=sampling_rate)  # Load audio with a specific sampling rate    \n",
    "    SI1=SIZ*(len(y1)//SIZ)\n",
    "    SI2=SIZ*(len(y2)//SIZ)\n",
    "    SI=min(SI1,SI2)\n",
    "    summ+=(SI//SIZ)\n",
    "    print(summ)\n",
    "\n",
    "    #print(y1.mean(),y2.mean())\n",
    "    audio_files = [y1[i:i+SIZ] for i in range(0, SI-30000, 10000)]  # This splits the audio into chunks\n",
    "\n",
    "    # Process each audio file to match the model's expected input format\n",
    "    inputs = [processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values for audio in audio_files]\n",
    "    segments1.extend(inputs)\n",
    "    audio_files = [y2[i:i+SIZ] for i in range(0, SI-30000, 10000)]  # This splits the audio into chunks\n",
    "\n",
    "    # Process each audio file to match the model's expected input format\n",
    "    inputs = [processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values for audio in audio_files]\n",
    "    segments2.extend(inputs)\n",
    "\n",
    "output_batch = torch.stack(segments2)\n",
    "input_batch = torch.stack(segments1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "combined_dataset = TensorDataset(input_batch, output_batch)\n",
    "total_size = len(combined_dataset)\n",
    "train_size = int(total_size * 0.8)  # e.g., 80% for training\n",
    "val_size = total_size - train_size  # the rest for validation\n",
    "indices = torch.arange(total_size)\n",
    "t_indices = indices[:train_size]\n",
    "v_indices = indices[train_size:]\n",
    "train_subset = Subset(combined_dataset, t_indices)\n",
    "val_subset = Subset(combined_dataset, v_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Data2VecAudioDecoder(nn.Module):\n",
    "    def __init__(self, encoder,seq_len):\n",
    "        super(Data2VecAudioDecoder, self).__init__()\n",
    "        self.encoder = encoder  # Storing the encoder reference\n",
    "        self.ratio=int(seq_len/10000)\n",
    "\n",
    "        # Projecting to a dimension that matches the expected input for the transposed convolutions\n",
    "        self.initial_projection = nn.Linear(768, 512 )  # Assuming we expand along the sequence length dimension\n",
    "\n",
    "        # Transposed convolutional layers to gradually upscale the temporal dimension and downscale feature dimension\n",
    "        self.conv_transpose_layers = nn.ModuleList([\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # Increasing length\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # Further increasing length\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=5, stride=2, padding=2, output_padding=1)  # Final layer to single channel\n",
    "        ])\n",
    "\n",
    "        # Correct LayerNorm configurations assuming output from each conv transpose layer\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm([512, 62*self.ratio]), nn.LayerNorm([512, 124*self.ratio]), nn.LayerNorm([256, 248*self.ratio]),\n",
    "            nn.LayerNorm([128, 496*self.ratio]), nn.LayerNorm([64, 992*self.ratio]), nn.LayerNorm([32, 1984*self.ratio]),\n",
    "            nn.LayerNorm([16, 3968*self.ratio]), nn.LayerNorm([1, 7936*self.ratio])  # Final expected size might need adjustment\n",
    "        ])\n",
    "\n",
    "        self.activations = nn.ModuleList([nn.GELU() for _ in range(8)])  # Activation functions for each layer\n",
    "        self.Final_projection = nn.Linear(self.ratio*7936,10000*self.ratio)  # Assuming we expand along the sequence length dimension\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder processing & extracting the last hidden state\n",
    "        x = self.encoder(x, output_hidden_states=True).last_hidden_state\n",
    "        x = self.initial_projection(x)\n",
    "\n",
    "        x = x.view(x.shape[0], x.shape[2], x.shape[1])  # Reshape to match the transposed convolution expectations\n",
    "        # Sequentially apply transposed convolutions, layer norms, and activations\n",
    "        for conv, norm, activation in zip(self.conv_transpose_layers, self.layer_norms, self.activations):\n",
    "            x = conv(x)\n",
    "            x = norm(x)\n",
    "            x = activation(x)\n",
    "\n",
    "\n",
    "        x=self.flatten(x)\n",
    "        return self.Final_projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Data2VecAudioDecoder(encoder, seq_len=SIZ).to(device)  # Model initialization\n",
    "\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('/home/yasser.attia/Music_Project/path_to_my_model_weights.pth')\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "for epoch in range(50):\n",
    "            train_loss=0\n",
    "            idx=0\n",
    "            for orig, cover in train_loader:  # Iterate over the training data\n",
    "                orig = orig.to(device)\n",
    "                cover = cover.to(device)\n",
    "                orig=orig.squeeze(1)\n",
    "                cover=cover.squeeze(1)\n",
    "\n",
    "                y_pred = model(orig)\n",
    "                loss = criterion(y_pred, cover)\n",
    "                train_loss+=loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                idx+=1\n",
    "            print(f'Epoch: {epoch+1} Training Loss: {train_loss/len(train_loader):.7f}')\n",
    "\n",
    "\n",
    "            #if epoch%100==999:\n",
    "                #scheduler.step()\n",
    "            if epoch%10==0:\n",
    "                model.eval()\n",
    "                val_loss=0\n",
    "                idx=0\n",
    "                for orig, cover in val_loader:  # Iterate over the training data\n",
    "                    orig = orig.to(device)\n",
    "                    cover = cover.to(device)\n",
    "                    orig=orig.squeeze(1)\n",
    "                    cover=cover.squeeze(1)\n",
    "\n",
    "                    y_pred = model(orig)\n",
    "                    loss = criterion(y_pred, cover)\n",
    "                    val_loss+=loss.item()\n",
    "                    idx+=1\n",
    "                model.train()\n",
    "                print(f'Epoch: {epoch+1}  Validation Loss: {val_loss/len(val_loader):.7f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LI=[\"VØJ, Narvent - Memory Reboot (4K Music Video)\",\"Üsküdar_a Gider İken\",\"Symphony\",\"Toss a Coin to Your Witcher\",\"Smooth Criminal\",\"The good the bad and the ugly\"\n",
    "    ,\"Stronghold - CastleJam\",\"Señorita\",\"Morrison Merrily Kissed The Quaker\",\"Saad Lamjarred - ADDA ELKALAM\", \"Ahd Al Asdikaa\",\"All I Want - Kodaline\",\"Amr Diab - Qusad Einy\",\"Bella Ciao\"\n",
    "    ,\"Fly Me To The Moon\",\"Hamza Namira - Dari Ya Alby\",\"Ladies of the Woods\", \"Lady Gaga - Shallow\",\"Lady Gaga- Rain On Me\",\"Memories - Maroon 5\",\"Attention\",\"GOT\"\n",
    "    ,\"Skrillex - Scary Monsters And Nice Sprites\",\"They_Donot_Care_About_Us\",\"The Misty Mountains Cold - The Hobbit\",\"Fluxxwave\",\"Aria Math (Minecraft) - C418\",\"Mareux - The Perfect Girl\" \n",
    "    ,\"METAMORPHOSIS\",\"Dusk Till Dawn - ZAYN Ft. Sia\",\"Let It Happen - Tame Impala\",\"Interstellar Main Theme - Hans Zimmer\",\"As It Was - Harry Styles\",\"Crystal Castles - KEROSEN\",\"Life in Rio\"    \n",
    "    ,\"Daft Punk - Instant Crush\",\"Fine Line - Harry Styles\",\"Numb - Linkin Park\",\"Sparky Deathcap - September\",\"SLOW DANCING IN THE DARK\",\"Faded - Alan Walker\",\"John Legend - All of Me\"\n",
    "    ,\"Nour El Ain - Amr Diab\",\"Rockabye\",\"Sia - The Greatest\",\"Sidi Mansour\" ,\"The Last Of The Mohicans\",\"The Sound Of Silence\",\"Dance Monkey\",\"Desert Rose - Sting & Cheb Mami\"\n",
    "    ,\"Despacito\",\"Don_t Stop _Til You Get Enough\",\"Ed Sheeran - Perfect\",\"Ed Sheeran - Shape Of You\",\"HOME - Resonance\",\"Interstellar Theme\"\n",
    "    ,\"Adele - Hello\",\"Aicha - Cheb Khaled\",\"Blinding Lights\",\"Charlie Puth - We Don_t Talk Anymore\",\"Cheap Thrills - Sia (Oud cover)\",\"Closer - The Chainsmokers\",\"Coffin Dance\",\n",
    "    \"Story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=256)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    return mfcc_mean.reshape(1, -1)\n",
    "\n",
    "# Paths to your audio files\n",
    "Summ=0\n",
    "for song in LI:\n",
    "    print(song)\n",
    "    audio_path_ref = \"/home/yasser.attia/Music_Project/mm/\"+song+\"_Arabic.wav\"\n",
    "    audio_path_gen = \"/home/yasser.attia/Music_Project/out_dir2/\"+song+\".wav\"\n",
    "\n",
    "    # Extract features\n",
    "    features_ref = extract_features(audio_path_ref)\n",
    "    features_gen = extract_features(audio_path_gen)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(features_ref, features_gen)\n",
    "    Summ+=similarity[0][0]\n",
    "    print(f'Cosine similarity: {similarity[0][0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average similarty: \", Summ/len(LI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in LI:\n",
    "        LII=[ song]\n",
    "\n",
    "        import soundfile as sf\n",
    "\n",
    "        dir=\"/home/yasser.attia/Music_Project/mm/\"\n",
    "        sampling_rate=16000\n",
    "\n",
    "        output_dir = \"/home/yasser.attia/Music_Project/out_dir3/\" \n",
    "        for pa in LII:\n",
    "                file_inp=dir+pa+\".wav\"\n",
    "                output_path=output_dir+pa+\".wav\"\n",
    "                ar_files=dir+pa+\"_Arabic.wav\"\n",
    "                print(pa)\n",
    "\n",
    "                y1, sr = librosa.load(file_inp, sr=sampling_rate)  # Load audio with a specific sampling rate\n",
    "                y2, sr2 = librosa.load(ar_files, sr=sampling_rate)  # Load audio with a specific sampling rate\n",
    "\n",
    "                audio_files = [y1[i:i+SIZ] for i in range(0, len(y1)-30000, 30000)]  # This splits the audio into chunks\n",
    "                inputs = [processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values for audio in audio_files]\n",
    "                input_batch = torch.stack(inputs).to(device)\n",
    "                input_batch=input_batch.squeeze(1)\n",
    "                y=[]\n",
    "                for i in input_batch:\n",
    "                        j=model(i.unsqueeze(0)) \n",
    "                        y.extend(j.cpu().detach().numpy().tolist()[0])\n",
    "        y=np.array(y)\n",
    "\n",
    "        D = librosa.stft(y2, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(abs(D), ref=np.max)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(S_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Spectrogram for '+pa+' oud couver Ground-truth')\n",
    "        plt.savefig('/home/yasser.attia/Music_Project/spectogram/'+pa+'_GT.png', dpi=300)  # Save as PNG with high resolution\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        n_fft = 2048  # Higher frame size\n",
    "        hop_length = 512  # Smaller hop length\n",
    "        D = librosa.stft(y1, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(abs(D), ref=np.max)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(S_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Spectrogram for original music of '+pa)\n",
    "        plt.savefig('/home/yasser.attia/Music_Project/spectogram/'+pa+'_inp.png', dpi=300)  # Save as PNG with high resolution\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        n_fft = 2048  # Higher frame size\n",
    "        hop_length = 512  # Smaller hop length\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(abs(D), ref=np.max)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(S_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='hz')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Spectrogram for Generated '+pa+' oud Cover')\n",
    "        plt.savefig('/home/yasser.attia/Music_Project/spectogram/'+pa+'_oud.png', dpi=300)  # Save as PNG with high resolution\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "dir=\"/home/yasser.attia/Music_Project/mm/\"\n",
    "sampling_rate=16000\n",
    "\n",
    "output_dir = \"/home/yasser.attia/Music_Project/out_dir3/\" \n",
    "for pa in LI:\n",
    "    file_inp=dir+pa+\".wav\"\n",
    "    output_path=output_dir+pa+\".wav\"\n",
    "    print(pa)\n",
    "\n",
    "    y1, sr = librosa.load(file_inp, sr=sampling_rate)  # Load audio with a specific sampling rate\n",
    "    audio_files = [y1[i:i+SIZ] for i in range(0, len(y1)-30000, 30000)]  # This splits the audio into chunks\n",
    "    inputs = [processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values for audio in audio_files]\n",
    "    input_batch = torch.stack(inputs).to(device)\n",
    "    input_batch=input_batch.squeeze(1)\n",
    "    y=[]\n",
    "    for i in input_batch:\n",
    "            j=model(i.unsqueeze(0)) \n",
    "            y.extend(j.cpu().detach().numpy().tolist()[0])\n",
    "    sf.write(output_path, y, samplerate=sampling_rate)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
